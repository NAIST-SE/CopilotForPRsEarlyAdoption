{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "monthly-gates",
   "metadata": {},
   "source": [
    "# Import tokens from env folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51056b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "class QueryFailError(Exception):\n",
    "    pass\n",
    "with open('./env/tokens.txt') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "tokens = [{'token': token, 'reset': None} for token in lines]\n",
    "token_index = 0\n",
    "def get_header():\n",
    "    return {\n",
    "    \"Authorization\": f\"Bearer {tokens[token_index]['token']}\",\n",
    "  }\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-hanging",
   "metadata": {},
   "source": [
    "# set query method and function to deal with api limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_query(query, headers):\n",
    "    request = requests.post(URL, json={\"query\": query}, headers=headers)\n",
    "    if request.status_code != 200:\n",
    "        logger.error(f\"{request.text}\")\n",
    "        raise QueryFailError(\n",
    "    \"Query failed to run by returning code of {}. {}\".format(\n",
    "        request.status_code, query\n",
    "    )\n",
    ")\n",
    "    try:\n",
    "        response = json.JSONDecoder().decode(json.dumps(request.json(), sort_keys=True))\n",
    "    except:\n",
    "        logger.error(f\"{request.text}\")\n",
    "        raise QueryFailError(\n",
    "            \"Query failed to run by returning code of {}. {}\".format(\n",
    "                request.status_code, url\n",
    "            )\n",
    "        )\n",
    "    try:\n",
    "        return response['data']\n",
    "    except KeyError:\n",
    "        return response\n",
    "def search_limit(headers):\n",
    "    query = \"\"\"\n",
    "                    query {\n",
    "                      viewer {\n",
    "                        login\n",
    "                      }\n",
    "                      rateLimit {\n",
    "                        limit\n",
    "                        cost\n",
    "                        remaining\n",
    "                        resetAt\n",
    "                      }\n",
    "                }\n",
    "                \"\"\"\n",
    "    try:\n",
    "        resp = send_query(query, headers)\n",
    "        global tokens\n",
    "        global token_index\n",
    "        tokens[token_index]['reset'] = pd.to_datetime(resp['rateLimit']['resetAt'])\n",
    "        if resp['rateLimit']['remaining'] > 200:\n",
    "            return\n",
    "        else:\n",
    "            token_index = (token_index + 1) % len(tokens)\n",
    "            if tokens[token_index]['reset'] is None:\n",
    "                return\n",
    "            if tokens[token_index]['reset'] < datetime.datetime.now(datetime.timezone.utc):\n",
    "                return\n",
    "            else:\n",
    "                time.sleep((tokens[token_index]['reset'] - datetime.datetime.now(datetime.timezone.utc)).total_seconds())\n",
    "                print(f\"Token {token_index} is ready to use again.\")\n",
    "                return\n",
    "    except (QueryFailError,KeyError) as e:\n",
    "        logger.error('sleep in ' + str(datetime.datetime.now()) + str(e))\n",
    "        time.sleep(3600)\n",
    "\n",
    "URL = \"https://api.github.com/graphql\"\n",
    "    \n",
    "def send_query_with_retries(query, retries, headers):\n",
    "    request = requests.post(URL, json={\"query\": query}, headers=headers)\n",
    "    if request.status_code != 200:\n",
    "        for i in range(retries):\n",
    "            time.sleep(random.randint(1, 3))\n",
    "            request = requests.post(URL, json={\"query\": query}, headers=headers)\n",
    "            print(\"retrying in \", i +1)\n",
    "            if request.status_code == 200: \n",
    "                break\n",
    "            \n",
    "            elif i == retries - 1:\n",
    "                logger.error(f\"{request.text}\")\n",
    "                raise QueryFailError(\n",
    "            \"Query failed to run by returning code of {}. {}\".format(\n",
    "                request.status_code, query\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    try:\n",
    "        response = json.JSONDecoder().decode(json.dumps(request.json(), sort_keys=True))\n",
    "    except:\n",
    "        logger.error(f\"{request.text}\")\n",
    "        raise QueryFailError(\n",
    "            \"Query failed to run by returning code of {}. {}\".format(\n",
    "                request.status_code, url\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        return response['data']\n",
    "    except KeyError:\n",
    "        return response  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-keyboard",
   "metadata": {},
   "source": [
    "# Section 3.1 PRs Generated by Copilot for PRs - collection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_edits_queries = []\n",
    "failed_experiences_queries = []\n",
    "edit_contents = pd.DataFrame(columns = ['repoName','number','editor','diff','editedAt','id', 'afterCommitSha','afterNumberOfCommits','afterPeriod'])\n",
    "failed_comments_queries = []\n",
    "failed_prs_queries = []\n",
    "prs_df = pd.DataFrame(columns = ['repoName','number','title','body','repoLanguage', 'repoCreatedAt', 'forkCount', 'stargazerCount', 'repoAge', 'createdAt',\n",
    "                                 'mergedAt','url','state','lastEditedAt', 'firstEditedAtBycopilot4prs','closedAt',\n",
    "                                 'updatedAt','deletions','additions','changedFiles','commentsTotalCount',\n",
    "                                'author','commitsTotalCount','prExperience','isMember'])\n",
    "comments_df = pd.DataFrame(columns = ['repoName','number','author','comment','id'])\n",
    "start_dt = datetime.datetime(2019, 2, 14)\n",
    "temp_start_dt = start_dt\n",
    "end_dt = datetime.datetime(2023, 9, 1)\n",
    "temp_end_dt = end_dt\n",
    "while temp_start_dt != end_dt:\n",
    "    search_limit(get_header())\n",
    "    query = '''\n",
    "    {\n",
    "      search(\n",
    "        query: \"\"\" \"Generated by Copilot\" is:public is:pr in:body created:temp_start_dt..temp_end_dt\"\"\"\n",
    "        type: ISSUE\n",
    "        first: 0\n",
    "      ) {\n",
    "        issueCount\n",
    "      }\n",
    "    }'''.replace('temp_start_dt',temp_start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('temp_end_dt',temp_end_dt.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
    "    try:\n",
    "        results = send_query_with_retries(query,3,get_header())\n",
    "    except QueryFailError as e:\n",
    "        logger.error('Obtaining pr counts with ' + str(e))\n",
    "        failed_prs_queries.append(query)\n",
    "        temp_start_dt = temp_end_dt\n",
    "        temp_end_dt = end_dt\n",
    "        continue\n",
    "    issueCount = results['search']['issueCount']\n",
    "    print(temp_start_dt, temp_end_dt, issueCount)\n",
    "    if issueCount == 0: \n",
    "        temp_start_dt = temp_end_dt\n",
    "        temp_end_dt = end_dt\n",
    "    elif issueCount <= 1000:\n",
    "        search_limit(get_header())\n",
    "        query = '''\n",
    "             query{\n",
    "                search(\n",
    "                    query: \"\"\" \"Generated by Copilot\" is:public is:pr in:body created:temp_start_dt..temp_end_dt\"\"\"\n",
    "                    type: ISSUE\n",
    "                    first: 50\n",
    "                    ) {\n",
    "                    edges {\n",
    "                      node {\n",
    "                        ... on PullRequest {\n",
    "                          authorAssociation\n",
    "                          number\n",
    "                          title\n",
    "                          body\n",
    "                          repository {\n",
    "                            nameWithOwner\n",
    "                            primaryLanguage {\n",
    "                              name\n",
    "                            }\n",
    "                            createdAt\n",
    "                            forkCount\n",
    "                            stargazerCount\n",
    "                          }\n",
    "                          createdAt\n",
    "                          mergedAt\n",
    "                          url\n",
    "                          state\n",
    "                          lastEditedAt\n",
    "                          userContentEdits(first: 100) {\n",
    "                            edges {\n",
    "                              node {\n",
    "                                editor {\n",
    "                                  login\n",
    "                                }\n",
    "                                diff\n",
    "                                editedAt\n",
    "                                id\n",
    "                              }\n",
    "                            }\n",
    "                          }\n",
    "                          closedAt\n",
    "                          updatedAt\n",
    "                          deletions\n",
    "                          additions\n",
    "                          changedFiles\n",
    "                          totalCommentsCount\n",
    "                          comments(first: 50) {\n",
    "                            totalCount\n",
    "                            edges {\n",
    "                              node {\n",
    "                                body\n",
    "                                author {\n",
    "                                  login\n",
    "                                }\n",
    "                                id\n",
    "                              }\n",
    "                            }\n",
    "                            pageInfo {\n",
    "                              endCursor\n",
    "                              hasNextPage\n",
    "                              hasPreviousPage\n",
    "                              startCursor\n",
    "                            }\n",
    "                          }\n",
    "                          author {\n",
    "                            login\n",
    "                          }\n",
    "                          commits(first: 100) {\n",
    "                              edges {\n",
    "                              node {\n",
    "                                commit {\n",
    "                                  oid\n",
    "                                  committedDate\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                            totalCount\n",
    "                          }\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                    pageInfo {\n",
    "                      endCursor\n",
    "                      hasNextPage\n",
    "                      hasPreviousPage\n",
    "                      startCursor\n",
    "                    }\n",
    "                  }\n",
    "                }'''.replace('temp_start_dt',temp_start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('temp_end_dt',temp_end_dt.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
    "        try:\n",
    "            results = send_query_with_retries(query,3,get_header())\n",
    "        except QueryFailError as e:\n",
    "            logger.error('Obtaining pr with ' + str(e))\n",
    "            failed_prs_queries.append(query)\n",
    "            temp_start_dt = temp_end_dt\n",
    "            temp_end_dt = end_dt\n",
    "            continue\n",
    "        for pr in results['search']['edges']:\n",
    "            editor_names = [editing['node']['editor']['login'] if editing['node']['editor'] is not None else 'ghost' for editing in pr['node']['userContentEdits']['edges']]\n",
    "            # Note now the copilot4prs bot has became ghost, change 'copilot4prs' to 'ghost' due to https://gist.github.com/idan/325676d192b32f169b032fde2d866c2c#github-next--technical-preview-sunsets\n",
    "            if ('copilot4prs' not in editor_names) or (pr['node']['number'] in list(prs_df.loc[prs_df['repoName'] == pr['node']['repository']['nameWithOwner']]['number'])): continue\n",
    "            committed_dates = [pd.to_datetime(commit['node']['commit']['committedDate']) for commit in pr['node']['commits']['edges']]\n",
    "            committed_shas = [commit['node']['commit']['oid'] for commit in pr['node']['commits']['edges']]\n",
    "            firstEditedAtBycopilot4prs = None\n",
    "            editings = pr['node']['userContentEdits']['edges']\n",
    "            editings.reverse()\n",
    "            for editing in editings:\n",
    "                editor_name = editing['node']['editor']['login'] if editing['node']['editor'] is not None else 'ghost'\n",
    "                after_commit_sha = None\n",
    "                after_number_of_commits = 0\n",
    "                after_period = None\n",
    "                for index, commited_date in enumerate(committed_dates):\n",
    "                    if pd.to_datetime(editing['node']['editedAt']) > commited_date:\n",
    "                        after_commit_sha = committed_shas[index]\n",
    "                        after_number_of_commits = index + 1\n",
    "                        after_period = (pd.to_datetime(editing['node']['editedAt']) - commited_date).total_seconds()/3600\n",
    "                edit_contents = pd.concat([edit_contents,pd.DataFrame([\n",
    "                    [pr['node']['repository']['nameWithOwner'],pr['node']['number'], editor_name, editing['node']['diff'], editing['node']['editedAt'], editing['node']['id'], after_commit_sha, after_number_of_commits, after_period]],\n",
    "                    columns=edit_contents.columns)], ignore_index=True)\n",
    "                if editor_name == 'copilot4prs' and firstEditedAtBycopilot4prs is None:\n",
    "                    firstEditedAtBycopilot4prs = editing['node']['editedAt']\n",
    "            pr_author = pr['node']['author']['login'] if pr['node']['author'] is not None else 'ghost'\n",
    "            author_association = pr['node']['authorAssociation']\n",
    "            is_member = author_association in ['MEMBER','OWNER']\n",
    "            # search author experince\n",
    "            search_limit(get_header())\n",
    "            query = '''\n",
    "                query{\n",
    "              search(\n",
    "                query: \"\"\"\n",
    "                repo:repo_name author:pr_author is:pr created:<pr_created\n",
    "                \"\"\"\n",
    "                type: ISSUE\n",
    "                first: 0\n",
    "              ) {\n",
    "                issueCount\n",
    "              }\n",
    "            }\n",
    "            '''.replace('repo_name', pr['node']['repository']['nameWithOwner']).replace('pr_author', pr_author).replace('pr_created', pr['node']['createdAt'])\n",
    "            try:\n",
    "                pr_experiences_results = send_query_with_retries(query,3,get_header())\n",
    "            except QueryFailError as e:\n",
    "                logger.error('Obtaining author experience with ' + str(e))\n",
    "                failed_experiences_queries.append(query)\n",
    "                continue  \n",
    "            preliminary_lang = pr['node']['repository']['primaryLanguage']['name'] if pr['node']['repository']['primaryLanguage'] is not None else None\n",
    "            repo_age = (pd.to_datetime(pr['node']['createdAt']) - pd.to_datetime(pr['node']['repository']['createdAt'])).total_seconds()/3600/24\n",
    "            prs_df = pd.concat([prs_df,pd.DataFrame([\n",
    "                [pr['node']['repository']['nameWithOwner'],pr['node']['number'], pr['node']['title'], pr['node']['body'],\n",
    "                 preliminary_lang, pr['node']['repository']['createdAt'], pr['node']['repository']['forkCount'], pr['node']['repository']['stargazerCount'], repo_age, pr['node']['createdAt'], pr['node']['mergedAt'], \n",
    "                 pr['node']['url'], pr['node']['state'], pr['node']['lastEditedAt'], firstEditedAtBycopilot4prs, pr['node']['closedAt'], pr['node']['updatedAt'],\n",
    "                 pr['node']['deletions'], pr['node']['additions'], pr['node']['changedFiles'], pr['node']['comments']['totalCount'],\n",
    "                  pr_author, pr['node']['commits']['totalCount'], pr_experiences_results['search']['issueCount'], is_member]],\n",
    "                columns=prs_df.columns)], ignore_index=True)\n",
    "            for comment in pr['node']['comments']['edges']:\n",
    "                comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                comments_df = pd.concat([comments_df,pd.DataFrame([\n",
    "                [pr['node']['repository']['nameWithOwner'], pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]],\n",
    "                columns=comments_df.columns)], ignore_index=True)\n",
    "            pr_comments = pr['node']['comments']\n",
    "            while pr_comments['pageInfo']['hasNextPage']:\n",
    "                comment_endcursor = pr_comments['pageInfo']['endCursor']\n",
    "                search_limit(get_header())\n",
    "                query = '''query{\n",
    "                  repository(name: \"repo_name\", owner: \"repo_owner\") {\n",
    "                    pullRequest(number: pr_number) {\n",
    "                      comments(first: 100, after: \"comment_endcursor\") {\n",
    "                        edges {\n",
    "                          node {\n",
    "                            author {\n",
    "                              login\n",
    "                            }\n",
    "                            body\n",
    "                            id\n",
    "                          }\n",
    "                        }\n",
    "                        pageInfo {\n",
    "                          endCursor\n",
    "                          hasNextPage\n",
    "                          hasPreviousPage\n",
    "                          startCursor\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                  }\n",
    "                }\n",
    "                '''.replace('repo_name',pr['node']['repository']['nameWithOwner'].split('/')[1]).replace(\n",
    "                    'repo_owner',pr['node']['repository']['nameWithOwner'].split('/')[0]).replace(\n",
    "                    'pr_number', str(pr['node']['number'])).replace('comment_endcursor',comment_endcursor)\n",
    "                try:\n",
    "                    comments_results = send_query_with_retries(query,3,get_header())\n",
    "                except QueryFailError as e:\n",
    "                    logger.error('Obtaining comment with ' + str(e))\n",
    "                    failed_comments_queries.append(query)\n",
    "                    break\n",
    "                pr_comments = comments_results['repository']['pullRequest']['comments']\n",
    "                for comment in pr_comments['edges']:\n",
    "                    comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                    comments_df = pd.concat([comments_df,pd.DataFrame([\n",
    "                    [pr['node']['repository']['nameWithOwner'], pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]],\n",
    "                    columns=comments_df.columns)], ignore_index=True)\n",
    "        while results['search']['pageInfo']['hasNextPage']:\n",
    "            endcursor = results['search']['pageInfo']['endCursor']\n",
    "            search_limit(get_header())\n",
    "            query = '''\n",
    "             query{\n",
    "                search(\n",
    "                    query: \"\"\" \"Generated by Copilot\" is:public is:pr in:body created:temp_start_dt..temp_end_dt\"\"\"\n",
    "                    type: ISSUE\n",
    "                    first: 50\n",
    "                    after:\"endcursor\"\n",
    "                    ) {\n",
    "                    edges {\n",
    "                      node {\n",
    "                        ... on PullRequest {\n",
    "                          authorAssociation\n",
    "                          number\n",
    "                          title\n",
    "                          body\n",
    "                          repository {\n",
    "                            nameWithOwner\n",
    "                            primaryLanguage {\n",
    "                              name\n",
    "                            }\n",
    "                            createdAt\n",
    "                            forkCount\n",
    "                            stargazerCount\n",
    "                          }\n",
    "                          createdAt\n",
    "                          mergedAt\n",
    "                          url\n",
    "                          state\n",
    "                          lastEditedAt\n",
    "                          userContentEdits(first: 100) {\n",
    "                            edges {\n",
    "                              node {\n",
    "                                editor {\n",
    "                                  login\n",
    "                                }\n",
    "                                diff\n",
    "                                editedAt\n",
    "                                id\n",
    "                              }\n",
    "                            }\n",
    "                          }\n",
    "                          closedAt\n",
    "                          updatedAt\n",
    "                          deletions\n",
    "                          additions\n",
    "                          changedFiles\n",
    "                          comments(first: 50) {\n",
    "                            totalCount\n",
    "                            edges {\n",
    "                              node {\n",
    "                                body\n",
    "                                author {\n",
    "                                  login\n",
    "                                }\n",
    "                                id\n",
    "                              }\n",
    "                            }\n",
    "                            pageInfo {\n",
    "                              endCursor\n",
    "                              hasNextPage\n",
    "                              hasPreviousPage\n",
    "                              startCursor\n",
    "                            }\n",
    "                          }\n",
    "                          author {\n",
    "                            login\n",
    "                          }\n",
    "                          commits(first: 100) {\n",
    "                              edges {\n",
    "                              node {\n",
    "                                commit {\n",
    "                                  oid\n",
    "                                  committedDate\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                            totalCount\n",
    "                          }\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                    pageInfo {\n",
    "                      endCursor\n",
    "                      hasNextPage\n",
    "                      hasPreviousPage\n",
    "                      startCursor\n",
    "                    }\n",
    "                  }\n",
    "                }'''.replace('endcursor',endcursor).replace('temp_start_dt',temp_start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('temp_end_dt',temp_end_dt.strftime('%Y-%m-%dT%H:%M:%SZ'))\n",
    "            try:\n",
    "                results = send_query_with_retries(query,3,get_header())\n",
    "            except QueryFailError as e:\n",
    "                logger.error('Obtaining pr with ' + str(e))\n",
    "                failed_prs_queries.append(query)\n",
    "                break\n",
    "            for pr in results['search']['edges']:\n",
    "                editor_names = [editing['node']['editor']['login'] if editing['node']['editor'] is not None else 'ghost' for editing in pr['node']['userContentEdits']['edges']]\n",
    "                # Note now the copilot4prs bot has became ghost, change 'copilot4prs' to 'ghost' due to https://gist.github.com/idan/325676d192b32f169b032fde2d866c2c#github-next--technical-preview-sunsets\n",
    "                if ('copilot4prs' not in editor_names) or (pr['node']['number'] in list(prs_df.loc[prs_df['repoName'] == pr['node']['repository']['nameWithOwner']]['number'])): continue\n",
    "                committed_dates = [pd.to_datetime(commit['node']['commit']['committedDate']) for commit in pr['node']['commits']['edges']]\n",
    "                committed_shas = [commit['node']['commit']['oid'] for commit in pr['node']['commits']['edges']]\n",
    "                firstEditedAtBycopilot4prs = None\n",
    "                editings = pr['node']['userContentEdits']['edges']\n",
    "                editings.reverse()\n",
    "                for editing in editings:\n",
    "                    editor_name = editing['node']['editor']['login'] if editing['node']['editor'] is not None else 'ghost'\n",
    "                    after_commit_sha = None\n",
    "                    after_number_of_commits = 0\n",
    "                    after_period = None\n",
    "                    for index, commited_date in enumerate(committed_dates):\n",
    "                        if pd.to_datetime(editing['node']['editedAt']) > commited_date:\n",
    "                            after_commit_sha = committed_shas[index]\n",
    "                            after_number_of_commits = index + 1\n",
    "                            after_period = (pd.to_datetime(editing['node']['editedAt']) - commited_date).total_seconds()/3600\n",
    "                    edit_contents = pd.concat([edit_contents,pd.DataFrame([\n",
    "                        [pr['node']['repository']['nameWithOwner'],pr['node']['number'], editor_name, editing['node']['diff'], editing['node']['editedAt'], editing['node']['id'], after_commit_sha, after_number_of_commits, after_period]],\n",
    "                        columns=edit_contents.columns)], ignore_index=True)\n",
    "                    if editor_name == 'copilot4prs' and firstEditedAtBycopilot4prs is None:\n",
    "                        firstEditedAtBycopilot4prs = editing['node']['editedAt']\n",
    "                pr_author = pr['node']['author']['login'] if pr['node']['author'] is not None else 'ghost'\n",
    "                author_association = pr['node']['authorAssociation']\n",
    "                is_member = author_association in ['MEMBER','OWNER']\n",
    "                # search author experince\n",
    "                search_limit(get_header())\n",
    "                query = '''\n",
    "                    query{\n",
    "                  search(\n",
    "                    query: \"\"\"\n",
    "                    repo:repo_name author:pr_author is:pr created:<pr_created\n",
    "                    \"\"\"\n",
    "                    type: ISSUE\n",
    "                    first: 0\n",
    "                  ) {\n",
    "                    issueCount\n",
    "                  }\n",
    "                }\n",
    "                '''.replace('repo_name', pr['node']['repository']['nameWithOwner']).replace('pr_author', pr_author).replace('pr_created', pr['node']['createdAt'])\n",
    "                try:\n",
    "                    pr_experiences_results = send_query_with_retries(query,3,get_header())\n",
    "                except QueryFailError as e:\n",
    "                    logger.error('Obtaining author experience with ' + str(e))\n",
    "                    failed_experiences_queries.append(query)\n",
    "                    continue  \n",
    "                preliminary_lang = pr['node']['repository']['primaryLanguage']['name'] if pr['node']['repository']['primaryLanguage'] is not None else None\n",
    "                repo_age = (pd.to_datetime(pr['node']['createdAt']) - pd.to_datetime(pr['node']['repository']['createdAt'])).total_seconds()/3600/24\n",
    "                prs_df = pd.concat([prs_df,pd.DataFrame([\n",
    "                    [pr['node']['repository']['nameWithOwner'],pr['node']['number'], pr['node']['title'], pr['node']['body'],\n",
    "                     preliminary_lang, pr['node']['repository']['createdAt'], pr['node']['repository']['forkCount'], pr['node']['repository']['stargazerCount'], repo_age, pr['node']['createdAt'], pr['node']['mergedAt'], \n",
    "                     pr['node']['url'], pr['node']['state'], pr['node']['lastEditedAt'], firstEditedAtBycopilot4prs, pr['node']['closedAt'], pr['node']['updatedAt'],\n",
    "                     pr['node']['deletions'], pr['node']['additions'], pr['node']['changedFiles'], pr['node']['comments']['totalCount'],\n",
    "                      pr_author, pr['node']['commits']['totalCount'], pr_experiences_results['search']['issueCount'], is_member]],\n",
    "                    columns=prs_df.columns)], ignore_index=True)\n",
    "                for comment in pr['node']['comments']['edges']:\n",
    "                    comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                    comments_df = pd.concat([comments_df,pd.DataFrame([\n",
    "                    [pr['node']['repository']['nameWithOwner'], pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]],\n",
    "                    columns=comments_df.columns)], ignore_index=True)\n",
    "                pr_comments = pr['node']['comments']\n",
    "                while pr_comments['pageInfo']['hasNextPage']:\n",
    "                    comment_endcursor = pr_comments['pageInfo']['endCursor']\n",
    "                    search_limit(get_header())\n",
    "                    query = '''query{\n",
    "                      repository(name: \"repo_name\", owner: \"repo_owner\") {\n",
    "                        pullRequest(number: pr_number) {\n",
    "                          comments(first: 100, after: \"comment_endcursor\") {\n",
    "                            edges {\n",
    "                              node {\n",
    "                                author {\n",
    "                                  login\n",
    "                                }\n",
    "                                body\n",
    "                                id\n",
    "                              }\n",
    "                            }\n",
    "                            pageInfo {\n",
    "                              endCursor\n",
    "                              hasNextPage\n",
    "                              hasPreviousPage\n",
    "                              startCursor\n",
    "                            }\n",
    "                          }\n",
    "                        }\n",
    "                      }\n",
    "                    }\n",
    "                    '''.replace('repo_name',pr['node']['repository']['nameWithOwner'].split('/')[1]).replace(\n",
    "                        'repo_owner',pr['node']['repository']['nameWithOwner'].split('/')[0]).replace(\n",
    "                        'pr_number', str(pr['node']['number'])).replace('comment_endcursor',comment_endcursor)\n",
    "                    try:\n",
    "                        comments_results = send_query_with_retries(query,3,get_header())\n",
    "                    except QueryFailError as e:\n",
    "                        logger.error('Obtaining comment with ' + str(e))\n",
    "                        failed_comments_queries.append(query)\n",
    "                        break\n",
    "                    pr_comments = comments_results['repository']['pullRequest']['comments']\n",
    "                    for comment in pr_comments['edges']:\n",
    "                        comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                        comments_df = pd.concat([comments_df,pd.DataFrame([\n",
    "                        [pr['node']['repository']['nameWithOwner'], pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]],\n",
    "                        columns=comments_df.columns)], ignore_index=True)\n",
    "        temp_start_dt = temp_end_dt\n",
    "        temp_end_dt = end_dt\n",
    "    else:\n",
    "        temp_end_dt = temp_start_dt + (temp_end_dt - temp_start_dt)/2\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-insider",
   "metadata": {},
   "source": [
    "# Section 3.1 PRs Generated by Copilot for PRs - Identifying Obsolete Uses of Copilot for PRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "firstEditedAtBycopilot4prs = pd.to_datetime(prs_df['firstEditedAtBycopilot4prs'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(firstEditedAtBycopilot4prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opend_prs_df = prs_df.loc[prs_df['closedAt'].isnull()].copy()\n",
    "closed_prs_df = prs_df.loc[~prs_df['closedAt'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "opend_prs_df.loc[:,'isObsolete'] =  opend_prs_df.apply(lambda row: (pd.to_datetime(row['createdAt']) < firstEditedAtBycopilot4prs) , axis=1)\n",
    "closed_prs_df.loc[:,'isObsolete'] = closed_prs_df.apply(lambda row: (pd.to_datetime(row['closedAt']) < pd.to_datetime(row['firstEditedAtBycopilot4prs'])) | (pd.to_datetime(row['createdAt']) < firstEditedAtBycopilot4prs) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df = pd.concat([opend_prs_df, closed_prs_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a232702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df[prs_df['isObsolete'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fe991c-43a4-4f20-bafd-c400ff6e6425",
   "metadata": {},
   "source": [
    "# Section 3.1 PRs Generated by Copilot for PRs - Excluding PRs Submitted by Bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "bots = pd.read_csv('../data/groundtruthbots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df = comments_df[(~comments_df['author'].str.endswith('bot')) & (~comments_df['author'].isin(list(bots.loc[bots['type'] == \"Bot\"]['account'])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-sunglasses",
   "metadata": {},
   "source": [
    "# Section 4.2 Casual Inference - Calculate the metadata comments, type, size, review time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_comments = []\n",
    "author_comments = []\n",
    "reviewers_comments = []\n",
    "reviewers_counts = []\n",
    "desc_lens = []\n",
    "pr_sizes = []\n",
    "review_times = []\n",
    "is_generated_by_bots = []\n",
    "for index, row in prs_df.iterrows():\n",
    "    is_generated_by_bots.append(row['author'].endswith('bot') or row['author'] in list(bots.loc[bots['type'] == \"Bot\"]['account']))\n",
    "    comments = cleaned_comments_df.loc[(cleaned_comments_df['repoName'] == row['repoName']) & (cleaned_comments_df['number'] == row['number'])]\n",
    "    author_comments.append(comments.loc[comments['author'] == row['author']].shape[0])\n",
    "    reviewers_comments.append(comments.loc[comments['author'] != row['author']].shape[0])\n",
    "    total_comments.append(comments.shape[0])\n",
    "    reviewers_counts.append(len(set(list(comments.loc[comments['author'] != row['author']]['author']))))\n",
    "    desc_lens.append(len(re.sub(r'[^A-Za-z0-9\\']+',' ', row['body'])))\n",
    "    pr_sizes.append(row['deletions'] + row['additions'])\n",
    "    if row['closedAt'] is not None:\n",
    "        review_times.append((pd.to_datetime(row['closedAt']) - pd.to_datetime(row['createdAt'])).total_seconds()/3600)\n",
    "    else:\n",
    "        review_times.append(None)\n",
    "prs_df['commentsTotalCount'] = total_comments\n",
    "prs_df['authorComments'] = author_comments\n",
    "prs_df['reviewersComments'] = reviewers_comments\n",
    "prs_df['reviewersTotalCount'] = reviewers_counts\n",
    "prs_df['bodyLength'] = desc_lens\n",
    "prs_df['prSize'] = pr_sizes\n",
    "prs_df['reviewTime'] = review_times\n",
    "prs_df['isGeneratedByBots'] = is_generated_by_bots\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eeabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df['repoName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1320d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df.loc[(prs_df['isObsolete'] == False) & ((prs_df['isGeneratedByBots'] == False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9fada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df.loc[(prs_df['isObsolete'] == False) & ((prs_df['isGeneratedByBots'] == False))]['repoName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df.loc[(prs_df['isObsolete'] == False) & ((prs_df['isGeneratedByBots'] == False)) & (prs_df['state'].isin(['MERGED', 'CLOSED']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae74734-996a-4c18-8996-7076c2453fe0",
   "metadata": {},
   "source": [
    "# Section 3.3 Revisions of PR Descriptions Generated by Copilot for PRs - Collection of Edits on PR Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db7947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents = edit_contents.rename(columns={'diff': 'content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1def8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9db008-e48f-4ab5-9981-f65e62f49a27",
   "metadata": {},
   "source": [
    "# Section 3.3 Revisions of PR Descriptions Generated by Copilot for PRs - Exclusion of PRs Without Post-Copilot Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-emergency",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers = pd.DataFrame(columns = edit_contents.columns)\n",
    "urls = []\n",
    "number_pr = 0\n",
    "number_edits = 0\n",
    "for name, group in edit_contents.groupby(['repoName','number']):\n",
    "    pr = prs_df.loc[(prs_df['repoName'] == group.iloc[0]['repoName']) & (prs_df['number'] == group.iloc[0]['number'])]\n",
    "    if (not pr['isObsolete'].values) and (not pr['isGeneratedByBots'].values) and (pr['state'].values in ['MERGED', 'CLOSED']):\n",
    "        number_pr += 1\n",
    "        number_edits += group.shape[0]\n",
    "        first_edit_by_copilot = group.loc[group['editor'] == 'copilot4prs']['editedAt'].min()\n",
    "        last_edit_by_developer = group.loc[group['editor'] != 'copilot4prs']['editedAt'].max()\n",
    "        if first_edit_by_copilot < last_edit_by_developer:\n",
    "            edit_contents_developers = pd.concat([edit_contents_developers, group], ignore_index=True)\n",
    "            urls.extend([pr['url'].values[0]] * group.shape[0])\n",
    "print(number_pr, number_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers['url'] = urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "conditions = [\n",
    "    prs_df['body'].str.contains('doc|copyright|license', case=False, na=False),\n",
    "    prs_df['body'].str.contains('bug|fix|defect', case=False, na=False)\n",
    "]\n",
    "\n",
    "choices = ['Document', 'Bug']\n",
    "\n",
    "prs_df['purpose'] = np.select(conditions, choices, default='Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df.to_csv('../data/LLMPRs.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248e33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prs_df.loc[(prs_df['isObsolete'] == False) & (prs_df['state'].isin(['MERGED','CLOSED'])) & (prs_df['isGeneratedByBots'] == False) ].drop(columns=['repoName', 'number', 'title', 'body','repoCreatedAt','createdAt', 'mergedAt',\n",
    "       'url','lastEditedAt', 'firstEditedAtBycopilot4prs','closedAt', 'updatedAt','author','isObsolete','isGeneratedByBots']).to_csv('../data/treatment_metrics.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('../data/LLMPRsComments.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df.to_csv('../data/cleanedLLMPRsComments.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents.to_csv('../data/edit_contents.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers.to_csv('../data/edit_contents_developers.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-bouquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prs_df = prs_df.loc[(prs_df['isValid'] == True) & ((prs_df['isGeneratedByBots'] == False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841413e-eba8-4bba-8e06-4cc3af846d1b",
   "metadata": {},
   "source": [
    "# Section 3.2 PRs Not generated by Copilot for PRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_comments_queries = []\n",
    "failed_prs_queries = []\n",
    "failed_experiences_queries = []\n",
    "control_prs_df = pd.DataFrame(columns = ['repoName','number','title','body','repoLanguage', 'repoCreatedAt', 'forkCount', 'stargazerCount', 'repoAge', 'createdAt',\n",
    "                                 'mergedAt','url','state','lastEditedAt','closedAt',\n",
    "                                 'updatedAt','deletions','additions','changedFiles','commentsTotalCount',\n",
    "                                'author','commitsTotalCount','prExperience','isMember'])\n",
    "control_comments_df = pd.DataFrame(columns = ['repoName','number','author','comment','id'])\n",
    "start_dt = firstEditedAtBycopilot4prs\n",
    "end_dt = pd.to_datetime('2023-09-01T00:00:00Z')\n",
    "for name, group in valid_prs_df.groupby('repoName'):\n",
    "    preliminary_lang = group.iloc[0]['repoLanguage']\n",
    "    fork_count = group.iloc[0]['forkCount']\n",
    "    stargazer_count = group.iloc[0]['stargazerCount']\n",
    "    repo_created = group.iloc[0]['repoCreatedAt']\n",
    "    temp_start_dt = start_dt\n",
    "    temp_end_dt = end_dt\n",
    "    while temp_start_dt != end_dt:\n",
    "        search_limit(get_header())\n",
    "        \n",
    "        query = '''\n",
    "        {\n",
    "          search(\n",
    "            query: \"\"\" NOT \"Generated by Copilot\" repo:repo_name is:pr created:temp_start_dt..temp_end_dt\"\"\"\n",
    "            type: ISSUE\n",
    "            first: 0\n",
    "          ) {\n",
    "            issueCount\n",
    "          }\n",
    "        }'''.replace('temp_start_dt',temp_start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('temp_end_dt',temp_end_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('repo_name',name)\n",
    "        try:\n",
    "            results = send_query_with_retries(query,3,get_header())\n",
    "        except QueryFailError as e:\n",
    "            logger.error('Obtaining pr counts with ' + str(e))\n",
    "            failed_prs_queries.append(query)\n",
    "            temp_start_dt = temp_end_dt\n",
    "            temp_end_dt = end_dt\n",
    "            continue\n",
    "        issueCount = results['search']['issueCount']\n",
    "        print(name, temp_start_dt, temp_end_dt,issueCount)\n",
    "        if issueCount == 0: \n",
    "            temp_start_dt = temp_end_dt\n",
    "            temp_end_dt = end_dt\n",
    "        elif issueCount <= 1000:\n",
    "            search_limit(get_header())\n",
    "            query = '''\n",
    "                 query{\n",
    "                    search(\n",
    "                        query: \"\"\" NOT \"Generated by Copilot\" repo:repo_name is:pr created:temp_start_dt..temp_end_dt\"\"\"\n",
    "                        type: ISSUE\n",
    "                        first: 50\n",
    "                        ) {\n",
    "                        edges {\n",
    "                          node {\n",
    "                            ... on PullRequest {\n",
    "                              authorAssociation\n",
    "                              number\n",
    "                              title\n",
    "                              body\n",
    "                              createdAt\n",
    "                              mergedAt\n",
    "                              url\n",
    "                              state\n",
    "                              lastEditedAt\n",
    "                              closedAt\n",
    "                              updatedAt\n",
    "                              deletions\n",
    "                              additions\n",
    "                              changedFiles\n",
    "                              comments(first: 50) {\n",
    "                                totalCount\n",
    "                                edges {\n",
    "                                  node {\n",
    "                                    body\n",
    "                                    author {\n",
    "                                      login\n",
    "                                    }\n",
    "                                    id\n",
    "                                  }\n",
    "                                }\n",
    "                                pageInfo {\n",
    "                                  endCursor\n",
    "                                  hasNextPage\n",
    "                                  hasPreviousPage\n",
    "                                  startCursor\n",
    "                                }\n",
    "                              }\n",
    "                            \n",
    "                              author {\n",
    "                                login\n",
    "                              }\n",
    "                              commits {\n",
    "                                totalCount\n",
    "                              }\n",
    "                            }\n",
    "                          }\n",
    "                        }\n",
    "                        pageInfo {\n",
    "                          endCursor\n",
    "                          hasNextPage\n",
    "                          hasPreviousPage\n",
    "                          startCursor\n",
    "                        }\n",
    "                      }\n",
    "                    }'''.replace('temp_start_dt',temp_start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('temp_end_dt',temp_end_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('repo_name',name)\n",
    "            try:\n",
    "                results = send_query_with_retries(query,3,get_header())\n",
    "            except QueryFailError as e:\n",
    "                logger.error('Obtaining pr with ' + str(e))\n",
    "                failed_prs_queries.append(query)\n",
    "                temp_start_dt = temp_end_dt\n",
    "                temp_end_dt = end_dt\n",
    "                continue\n",
    "            for pr in results['search']['edges']:\n",
    "                if (pr['node']['number'] in list(prs_df.loc[prs_df['repoName'] == name]['number'])) or (pr['node']['number'] in list(control_prs_df.loc[control_prs_df['repoName'] == name]['number'])): continue\n",
    "                else:\n",
    "                    pr_author = pr['node']['author']['login'] if pr['node']['author'] is not None else 'ghost'\n",
    "                    author_association = pr['node']['authorAssociation']\n",
    "                    is_member = author_association in ['MEMBER','OWNER']\n",
    "                    # search author experince\n",
    "                    search_limit(get_header())\n",
    "                    query = '''\n",
    "                        query{\n",
    "                      search(\n",
    "                        query: \"\"\"\n",
    "                        repo:repo_name author:pr_author is:pr created:<pr_created\n",
    "                        \"\"\"\n",
    "                        type: ISSUE\n",
    "                        first: 0\n",
    "                      ) {\n",
    "                        issueCount\n",
    "                      }\n",
    "                    }\n",
    "                    '''.replace('repo_name', name).replace('pr_author', pr_author).replace('pr_created', pr['node']['createdAt'])\n",
    "                    try:\n",
    "                        pr_experiences_results = send_query_with_retries(query,3,get_header())\n",
    "                    except QueryFailError as e:\n",
    "                        logger.error('Obtaining author experience with ' + str(e))\n",
    "                        failed_experiences_queries.append(query)\n",
    "                        continue\n",
    "                    repo_age = (pd.to_datetime(pr['node']['createdAt']) - pd.to_datetime(repo_created)).total_seconds()/3600/24\n",
    "                    control_prs_df = pd.concat([control_prs_df,pd.DataFrame([\n",
    "                        [name, pr['node']['number'], pr['node']['title'], pr['node']['body'],\n",
    "                         preliminary_lang, repo_created, fork_count, stargazer_count, repo_age, pr['node']['createdAt'], pr['node']['mergedAt'], \n",
    "                         pr['node']['url'], pr['node']['state'], pr['node']['lastEditedAt'], pr['node']['closedAt'], pr['node']['updatedAt'],\n",
    "                         pr['node']['deletions'], pr['node']['additions'], pr['node']['changedFiles'], pr['node']['comments']['totalCount'],\n",
    "                         pr_author, pr['node']['commits']['totalCount'], pr_experiences_results['search']['issueCount'], is_member]],\n",
    "                        columns=control_prs_df.columns)], ignore_index=True)\n",
    "                    for comment in pr['node']['comments']['edges']:\n",
    "                        comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                        control_comments_df = pd.concat([control_comments_df,pd.DataFrame([\n",
    "                        [name, pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]],\n",
    "                        columns=control_comments_df.columns)], ignore_index=True)\n",
    "                    pr_comments = pr['node']['comments']\n",
    "                    while pr_comments['pageInfo']['hasNextPage']:\n",
    "                        comment_endcursor = pr_comments['pageInfo']['endCursor']\n",
    "                        search_limit(get_header())\n",
    "                        query = '''query{\n",
    "                          repository(name: \"repo_name\", owner: \"repo_owner\") {\n",
    "                            pullRequest(number: pr_number) {\n",
    "                              comments(first: 100, after: \"comment_endcursor\") {\n",
    "                                edges {\n",
    "                                  node {\n",
    "                                    author {\n",
    "                                      login\n",
    "                                    }\n",
    "                                    body\n",
    "                                    id\n",
    "                                  }\n",
    "                                }\n",
    "                                pageInfo {\n",
    "                                  endCursor\n",
    "                                  hasNextPage\n",
    "                                  hasPreviousPage\n",
    "                                  startCursor\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                          }\n",
    "                        }\n",
    "                        '''.replace('repo_name',name.split('/')[1]).replace(\n",
    "                            'repo_owner',name.split('/')[0]).replace(\n",
    "                            'pr_number', str(pr['node']['number'])).replace('comment_endcursor',comment_endcursor)\n",
    "                        try:\n",
    "                            comments_results = send_query_with_retries(query,3,get_header())\n",
    "                        except QueryFailError as e:\n",
    "                            logger.error('Obtaining comment with ' + str(e))\n",
    "                            failed_comments_queries.append(query)\n",
    "                            break\n",
    "                        pr_comments = comments_results['repository']['pullRequest']['comments']\n",
    "                        for comment in pr_comments['edges']:\n",
    "                            comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                            control_comments_df = pd.concat([control_comments_df,pd.DataFrame([\n",
    "                            [name, pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]],\n",
    "                            columns=control_comments_df.columns)], ignore_index=True)\n",
    "            while results['search']['pageInfo']['hasNextPage']:\n",
    "                endcursor = results['search']['pageInfo']['endCursor']\n",
    "                search_limit(get_header())\n",
    "                query = '''\n",
    "                 query{\n",
    "                    search(\n",
    "                        query: \"\"\" NOT \"Generated by Copilot\" repo:repo_name is:pr created:temp_start_dt..temp_end_dt\"\"\"\n",
    "                        type: ISSUE\n",
    "                        first: 50\n",
    "                        after:\"endcursor\"\n",
    "                        ) {\n",
    "                        edges {\n",
    "                          node {\n",
    "                            ... on PullRequest {\n",
    "                              authorAssociation\n",
    "                              number\n",
    "                              title\n",
    "                              body\n",
    "                              createdAt\n",
    "                              mergedAt\n",
    "                              url\n",
    "                              state\n",
    "                              lastEditedAt\n",
    "                              closedAt\n",
    "                              updatedAt\n",
    "                              deletions\n",
    "                              additions\n",
    "                              changedFiles\n",
    "                              comments(first: 50) {\n",
    "                                totalCount\n",
    "                                edges {\n",
    "                                  node {\n",
    "                                    body\n",
    "                                    author {\n",
    "                                      login\n",
    "                                    }\n",
    "                                    id\n",
    "                                  }\n",
    "                                }\n",
    "                                pageInfo {\n",
    "                                  endCursor\n",
    "                                  hasNextPage\n",
    "                                  hasPreviousPage\n",
    "                                  startCursor\n",
    "                                }\n",
    "                              }\n",
    "                              \n",
    "                              author {\n",
    "                                login\n",
    "                              }\n",
    "                              commits {\n",
    "                                totalCount\n",
    "                              }\n",
    "                            }\n",
    "                          }\n",
    "                        }\n",
    "                        pageInfo {\n",
    "                          endCursor\n",
    "                          hasNextPage\n",
    "                          hasPreviousPage\n",
    "                          startCursor\n",
    "                        }\n",
    "                      }\n",
    "                    }'''.replace('endcursor',endcursor).replace('temp_start_dt',temp_start_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('temp_end_dt',temp_end_dt.strftime('%Y-%m-%dT%H:%M:%SZ')).replace('repo_name',name)\n",
    "                try:\n",
    "                    results = send_query_with_retries(query,3,get_header())\n",
    "                except QueryFailError as e:\n",
    "                    logger.error('Obtaining pr with ' + str(e))\n",
    "                    failed_prs_queries.append(query)\n",
    "                    break\n",
    "                for pr in results['search']['edges']:\n",
    "                    if (pr['node']['number'] in list(prs_df.loc[prs_df['repoName'] == name]['number'])) or (pr['node']['number'] in list(control_prs_df.loc[control_prs_df['repoName'] == name]['number'])): continue\n",
    "                    else:\n",
    "                        pr_author = pr['node']['author']['login'] if pr['node']['author'] is not None else 'ghost'\n",
    "                        author_association = pr['node']['authorAssociation']\n",
    "                        is_member = author_association in ['MEMBER','OWNER']\n",
    "                        # search author experince\n",
    "                        search_limit(get_header())\n",
    "                        query = '''\n",
    "                            query{\n",
    "                          search(\n",
    "                            query: \"\"\"\n",
    "                            repo:repo_name author:pr_author is:pr created:<pr_created\n",
    "                            \"\"\"\n",
    "                            type: ISSUE\n",
    "                            first: 0\n",
    "                          ) {\n",
    "                            issueCount\n",
    "                          }\n",
    "                        }\n",
    "                        '''.replace('repo_name', name).replace('pr_author', pr_author).replace('pr_created', pr['node']['createdAt'])\n",
    "                        try:\n",
    "                            pr_experiences_results = send_query_with_retries(query,3,get_header())\n",
    "                        except QueryFailError as e:\n",
    "                            logger.error('Obtaining author experience with ' + str(e))\n",
    "                            failed_experiences_queries.append(query)\n",
    "                            continue \n",
    "                        repo_age = (pd.to_datetime(pr['node']['createdAt']) - pd.to_datetime(repo_created)).total_seconds()/3600/24\n",
    "                        control_prs_df = pd.concat([control_prs_df,pd.DataFrame([\n",
    "                            [name, pr['node']['number'], pr['node']['title'], pr['node']['body'],\n",
    "                             preliminary_lang, repo_created, fork_count, stargazer_count, repo_age, pr['node']['createdAt'], pr['node']['mergedAt'], \n",
    "                             pr['node']['url'], pr['node']['state'], pr['node']['lastEditedAt'], pr['node']['closedAt'], pr['node']['updatedAt'],\n",
    "                             pr['node']['deletions'], pr['node']['additions'], pr['node']['changedFiles'], pr['node']['comments']['totalCount'],\n",
    "                             pr_author, pr['node']['commits']['totalCount'], pr_experiences_results['search']['issueCount'], is_member]],\n",
    "                            columns=control_prs_df.columns)], ignore_index=True)\n",
    "                        for comment in pr['node']['comments']['edges']:\n",
    "                            comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                            control_comments_df = pd.concat([control_comments_df,pd.DataFrame([\n",
    "                            [name, pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]], \n",
    "                            columns=control_comments_df.columns)], ignore_index=True)\n",
    "                        pr_comments = pr['node']['comments']\n",
    "                        while pr_comments['pageInfo']['hasNextPage']:\n",
    "                            comment_endcursor = pr_comments['pageInfo']['endCursor']\n",
    "                            search_limit(get_header())\n",
    "                            query = '''query{\n",
    "                              repository(name: \"repo_name\", owner: \"repo_owner\") {\n",
    "                                pullRequest(number: pr_number) {\n",
    "                                  comments(first: 100, after: \"comment_endcursor\") {\n",
    "                                    edges {\n",
    "                                      node {\n",
    "                                        author {\n",
    "                                          login\n",
    "                                        }\n",
    "                                        body\n",
    "                                        id\n",
    "                                      }\n",
    "                                    }\n",
    "                                    pageInfo {\n",
    "                                      endCursor\n",
    "                                      hasNextPage\n",
    "                                      hasPreviousPage\n",
    "                                      startCursor\n",
    "                                    }\n",
    "                                  }\n",
    "                                }\n",
    "                              }\n",
    "                            }\n",
    "                            '''.replace('repo_name',name.split('/')[1]).replace(\n",
    "                                'repo_owner',name.split('/')[0]).replace(\n",
    "                                'pr_number', str(pr['node']['number'])).replace('comment_endcursor',comment_endcursor)\n",
    "                            try:\n",
    "                                comments_results = send_query_with_retries(query,3,get_header())\n",
    "                            except QueryFailError as e:\n",
    "                                logger.error('Obtaining comment with ' + str(e))\n",
    "                                failed_comments_queries.append(query)\n",
    "                                break\n",
    "                            pr_comments = comments_results['repository']['pullRequest']['comments']\n",
    "                            for comment in pr_comments['edges']:\n",
    "                                comment_author = comment['node']['author']['login'] if comment['node']['author'] is not None else 'ghost'\n",
    "                                control_comments_df = pd.concat([control_comments_df,pd.DataFrame([\n",
    "                                [name, pr['node']['number'], comment_author, comment['node']['body'], comment['node']['id']]],\n",
    "                                columns=control_comments_df.columns)], ignore_index=True)\n",
    "            temp_start_dt = temp_end_dt\n",
    "            temp_end_dt = end_dt\n",
    "        else:\n",
    "            temp_end_dt = temp_start_dt + (temp_end_dt - temp_start_dt)/2\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_control_comments_df = control_comments_df[(~control_comments_df['author'].str.endswith('bot')) & (~control_comments_df['author'].isin(list(bots.loc[bots['type'] == \"Bot\"]['account'])))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_control_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "control_total_comments = []\n",
    "control_author_comments = []\n",
    "control_reviewers_comments = []\n",
    "control_reviewers_counts = []\n",
    "control_desc_lens = []\n",
    "control_pr_sizes = []\n",
    "control_review_times = []\n",
    "control_is_generated_by_bots = []\n",
    "for index, row in control_prs_df.iterrows():\n",
    "    control_is_generated_by_bots.append(row['author'].endswith('bot') or row['author'] in list(bots.loc[bots['type'] == \"Bot\"]['account']))\n",
    "    comments = cleaned_control_comments_df.loc[(cleaned_control_comments_df['repoName'] == row['repoName']) & (cleaned_control_comments_df['number'] == row['number'])]\n",
    "    control_author_comments.append(comments.loc[comments['author'] == row['author']].shape[0])\n",
    "    control_reviewers_comments.append(comments.loc[comments['author'] != row['author']].shape[0])\n",
    "    control_total_comments.append(comments.shape[0])\n",
    "    control_reviewers_counts.append(len(set(list(comments.loc[comments['author'] != row['author']]['author']))))\n",
    "    control_desc_lens.append(len(re.sub(r'[^A-Za-z0-9\\']+',' ', row['body'])))\n",
    "    control_pr_sizes.append(row['deletions'] + row['additions'])\n",
    "    if row['closedAt'] is not None:\n",
    "        control_review_times.append((pd.to_datetime(row['closedAt']) - pd.to_datetime(row['createdAt'])).total_seconds()/3600)\n",
    "    else:\n",
    "        control_review_times.append(None)\n",
    "control_prs_df['commentsTotalCount'] = control_total_comments\n",
    "control_prs_df['authorComments'] = control_author_comments\n",
    "control_prs_df['reviewersComments'] = control_reviewers_comments\n",
    "control_prs_df['reviewersTotalCount'] = control_reviewers_counts\n",
    "control_prs_df['bodyLength'] = control_desc_lens\n",
    "control_prs_df['prSize'] = control_pr_sizes\n",
    "control_prs_df['reviewTime'] = control_review_times\n",
    "control_prs_df['isGeneratedByBots'] = control_is_generated_by_bots\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd81dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prs_df[control_prs_df['isGeneratedByBots'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb8172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prs_df[control_prs_df['isGeneratedByBots'] == False]['repoName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d47c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    control_prs_df['body'].str.contains('doc|copyright|license', case=False, na=False),\n",
    "    control_prs_df['body'].str.contains('bug|fix|defect', case=False, na=False)\n",
    "]\n",
    "\n",
    "choices = ['Document', 'Bug']\n",
    "\n",
    "control_prs_df['purpose'] = np.select(conditions, choices, default='Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prs_df.to_csv('../data/control_prs_df.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b4a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c389ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prs_df.loc[(control_prs_df['state'].isin(['MERGED','CLOSED'])) & (control_prs_df['isGeneratedByBots'] == False) ].drop(columns=['repoName', 'number', 'title', 'body','repoCreatedAt','createdAt', 'mergedAt',\n",
    "       'url','lastEditedAt', 'closedAt', 'updatedAt','author','isGeneratedByBots']).to_csv('../data/control_metrics.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_comments_df.to_csv('../data/control_comments_df.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_control_comments_df.to_csv('../data/cleaned_control_comments_df.csv',index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_prs_df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prs_df[control_prs_df['isGeneratedByBots'] == False]['state'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
