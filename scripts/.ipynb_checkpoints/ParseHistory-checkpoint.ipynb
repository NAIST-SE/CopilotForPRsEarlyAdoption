{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbed950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import subprocess\n",
    "edit_contents_developers = pd.read_csv('../data/edit_contents_developers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce966b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers.groupby(by=[\"repoName\", \"number\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a69e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers['content'] = edit_contents_developers['content'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_diff(lines):\n",
    "    modified_lines = {'added': {}, 'deleted': {}}\n",
    "    count_deletions = 0\n",
    "    count_additions = 0\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        line = line.rstrip()\n",
    "        count_deletions += 1\n",
    "        count_additions += 1\n",
    "        if line.startswith('@@'):\n",
    "            count_deletions, count_additions = get_line_numbers(line)\n",
    "\n",
    "        elif line.startswith('-'):\n",
    "            modified_lines['deleted'].update({count_deletions: line[1:]})\n",
    "            count_additions -= 1\n",
    "        elif line.startswith('+'):\n",
    "            modified_lines['added'].update({count_additions: line[1:]})\n",
    "            count_deletions -= 1\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    return modified_lines\n",
    "def get_line_numbers(line):\n",
    "    token = line.split(\" \")\n",
    "    numbers_old_file = token[1]\n",
    "    numbers_new_file = token[2]\n",
    "    delete_line_number = int(numbers_old_file.split(\",\")[0].replace(\"-\", \"\")) - 1\n",
    "    additions_line_number = int(numbers_new_file.split(\",\")[0]) - 1\n",
    "    return delete_line_number, additions_line_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ce17a-e0e5-4363-ac00-2dfc6a35ef5d",
   "metadata": {},
   "source": [
    "# Section 3.3 Revisions of PR Descriptions Generated by Copilot for PRs - Identification of PRs with Post-Copilot Edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637cb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "added = []\n",
    "deleted = []\n",
    "ngroup = []\n",
    "summary_commands = []\n",
    "all_commands = []\n",
    "walkthrough_commands = []\n",
    "poem_commands = []\n",
    "is_edited_by_otherses = []\n",
    "is_rerun_commands = []\n",
    "is_edited = []\n",
    "changes = []\n",
    "i = 0\n",
    "for name, group in edit_contents_developers.groupby(['repoName', 'number']):\n",
    "    i += 1\n",
    "    edits = group.reset_index(drop=True)\n",
    "    contents_by_copilot = []\n",
    "    line_numbers = []\n",
    "    is_edited_by_others = False\n",
    "    is_rerun_command = False\n",
    "    is_second_copilot_command = False\n",
    "    summary_command = 0\n",
    "    all_command = 0\n",
    "    walkthrough_command = 0\n",
    "    poem_command = 0\n",
    "    for index, row in edits.iterrows():\n",
    "        \n",
    "        if index == 0: first = ''\n",
    "        else: first = edits.iloc[index - 1]['content'].replace('\\r','').replace(\"'\", \"'\\\\''\").strip()\n",
    "        second = row['content'].replace('\\r','').replace(\"'\", \"'\\\\''\").strip()\n",
    "        if second.count('\\n') == 0: second += '\\n'\n",
    "        process = os.popen(f\"git diff $(echo -e '{first}' | git hash-object -w --stdin) $(echo -e '{second}' | git hash-object -w --stdin)\")\n",
    "        diffs = process.read().split('\\n')[4:]\n",
    "\n",
    "        process.close()\n",
    "        modified_lines = parse_diff(diffs)\n",
    "        if row['editor'] == 'copilot4prs':\n",
    "            for number, line in list(modified_lines['added'].items()):\n",
    "                if line.startswith('- [x]') or line.startswith('- [ ]'):\n",
    "                    del modified_lines['added'][number]\n",
    "            for number, line in list(modified_lines['deleted'].items()):\n",
    "                if line.startswith('- [x]') or line.startswith('- [ ]'):\n",
    "                    del modified_lines['deleted'][number]      \n",
    "        second = second.replace(\"\"\"<!--\\ncopilot:all\\n-->\"\"\",'').replace(\"\"\"<!--\\ncopilot:summary\\n-->\"\"\",'').replace(\"\"\"<!--\\ncopilot:walkthrough\\n-->\"\"\",'').replace(\"\"\"<!--\\ncopilot:poem\\n-->\"\"\",'')\n",
    "        if row['editor'] != 'copilot4prs' and list(edits['editor']).count('copilot4prs') > 1 :\n",
    "            command = list(set(re.findall(r\"\\bcopilot:(all|summary|walkthrough|poem)\\b\", second)))\n",
    "            if 'summary' in command: summary_command += 1\n",
    "            if 'all' in command: all_command += 1\n",
    "            if 'walkthrough' in command: walkthrough_command += 1\n",
    "            if 'poem' in command: poem_command += 1\n",
    "        changes.append(modified_lines)\n",
    "        ngroup.append(i)\n",
    "        summary_commands.append(summary_command)\n",
    "        all_commands.append(all_command)\n",
    "        walkthrough_commands.append(walkthrough_command)\n",
    "        poem_commands.append(poem_command)\n",
    "        added_line_numbers = list(modified_lines['added'].keys())\n",
    "        deleted_line_numbers = list(modified_lines['deleted'].keys())\n",
    "\n",
    "        if len(contents_by_copilot) == 0:\n",
    "            if row['editor'] == 'copilot4prs':\n",
    "                contents_by_copilot.extend(modified_lines['added'].values())\n",
    "                line_numbers.extend(added_line_numbers)\n",
    "            is_edited.append(is_edited_by_others)\n",
    "            continue\n",
    "        while (not (len(added_line_numbers) == 0 and len(deleted_line_numbers) == 0)):\n",
    "#             print(added_line_numbers, deleted_line_numbers, line_numbers)\n",
    "            if len(added_line_numbers) == 0:\n",
    "                # pop up minimum deleted line, make rest of deleted line number - 1\n",
    "                deleted_line = deleted_line_numbers.pop(0)\n",
    "                deleted_line_numbers = [i - 1 for i in deleted_line_numbers]\n",
    "                if len(contents_by_copilot) == 0: continue\n",
    "                if deleted_line in line_numbers:\n",
    "                    if row['editor'] != 'copilot4prs':\n",
    "                        is_edited_by_others = True\n",
    "                    deleted_index = line_numbers.index(deleted_line)\n",
    "                    del(contents_by_copilot[deleted_index])\n",
    "                    del(line_numbers[deleted_index])\n",
    "                line_numbers = [i - 1 if i > deleted_line else i for i in line_numbers ]\n",
    "                continue\n",
    "            if len(deleted_line_numbers) == 0:\n",
    "                # pop up minimum deleted line, make rest of deleted line number - 1\n",
    "                added_line = added_line_numbers.pop(0)\n",
    "                if len(contents_by_copilot) == 0: continue\n",
    "                if row['editor'] == 'copilot4prs':\n",
    "                    line_numbers = [i + 1 if i >= added_line else i for i in line_numbers ]\n",
    "                    added_index = 0\n",
    "                    for number in line_numbers:\n",
    "                        if added_line < number:\n",
    "                            break\n",
    "                        else: added_index += 1\n",
    "                    contents_by_copilot.insert(added_index, modified_lines['added'][added_line])\n",
    "                    line_numbers.insert(added_index, added_line)\n",
    "                else:\n",
    "                    line_numbers = [i + 1 if i >= added_line else i for i in line_numbers ]\n",
    "                continue\n",
    "            # in each loop, pop up minimul line number for added and deleted\n",
    "            if added_line_numbers[0] < deleted_line_numbers[0]:\n",
    "                added_line = added_line_numbers.pop(0)\n",
    "                deleted_line_numbers = [i + 1 for i in deleted_line_numbers]\n",
    "                if len(contents_by_copilot) == 0: continue\n",
    "                if row['editor'] == 'copilot4prs':\n",
    "                    line_numbers = [i + 1 if i >= added_line else i for i in line_numbers ]\n",
    "                    added_index = 0\n",
    "                    for number in line_numbers:\n",
    "                        if added_line < number:\n",
    "                            break\n",
    "                        else: added_index += 1\n",
    "                    contents_by_copilot.insert(added_index , modified_lines['added'][added_line])\n",
    "                    line_numbers.insert(added_index , added_line)\n",
    "                else:\n",
    "                    line_numbers = [i + 1 if i >= added_line else i for i in line_numbers ]\n",
    "            # if added line number = deleted line number means the content exchange\n",
    "            elif added_line_numbers[0] == deleted_line_numbers[0]:\n",
    "                added_line = added_line_numbers.pop(0)\n",
    "                deleted_line = deleted_line_numbers.pop(0)\n",
    "                if len(line_numbers) == 0: continue\n",
    "                if added_line in line_numbers:\n",
    "                    added_index = line_numbers.index(added_line)\n",
    "                    if row['editor'] != 'copilot4prs':\n",
    "                        is_edited_by_others = True\n",
    "                        del(contents_by_copilot[added_index])\n",
    "                        del(line_numbers[added_index])\n",
    "                    else:\n",
    "                        contents_by_copilot[added_index] = modified_lines['added'][added_line]\n",
    "            else:\n",
    "                deleted_line = deleted_line_numbers.pop(0)\n",
    "                deleted_line_numbers = [i - 1 for i in deleted_line_numbers]\n",
    "                if len(contents_by_copilot) == 0: continue\n",
    "                if deleted_line in line_numbers:\n",
    "                    if row['editor'] != 'copilot4prs':\n",
    "                        is_edited_by_others = True\n",
    "                    deleted_index = line_numbers.index(deleted_line)\n",
    "                    del(contents_by_copilot[deleted_index])\n",
    "                    del(line_numbers[deleted_index])\n",
    "                line_numbers = [i - 1 if i > deleted_line else i for i in line_numbers ]\n",
    "        is_edited.append(is_edited_by_others)\n",
    "\n",
    "    is_edited_by_otherses.extend([is_edited_by_others] * edits.shape[0])\n",
    "    if summary_command > 1 or all_command > 1 or walkthrough_command > 1 or poem_command > 1:\n",
    "        is_rerun_command = True\n",
    "    is_rerun_commands.extend([is_rerun_command] * edits.shape[0])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b777dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers['group'] = ngroup\n",
    "edit_contents_developers['allCommands'] = all_commands\n",
    "edit_contents_developers['walkthroughCommands'] = walkthrough_commands\n",
    "edit_contents_developers['poemCommands'] = poem_commands\n",
    "edit_contents_developers['summaryCommand'] = summary_command\n",
    "edit_contents_developers['isRerunCommands'] = is_rerun_commands\n",
    "edit_contents_developers['isEditedByOtherses'] = is_edited_by_otherses\n",
    "edit_contents_developers['isThisDditingCopilot'] = is_edited\n",
    "edit_contents_developers['Changes'] = changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec69dff-9af1-4056-a98a-74507b051129",
   "metadata": {},
   "source": [
    "# Section 3.3 Revisions of PR Descriptions Generated by Copilot for PRs - Filtering PRs that Reapply Marker Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7697eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers[(edit_contents_developers['isRerunCommands'] == True)]['group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers[(edit_contents_developers['isRerunCommands'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_contents_developers[(edit_contents_developers['isEditedByOtherses'] == True)]['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a39524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/edit_contents_developers_with_diff.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_df = edit_contents_developers[(edit_contents_developers['isRerunCommands'] == False) & (edit_contents_developers['isEditedByOtherses'] == True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_df['group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_df['repoName'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cec6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('./env/tokens.txt') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "class QueryFailError(Exception):\n",
    "    pass\n",
    "tokens = [{'token': token, 'reset': None} for token in lines]\n",
    "token_index = 0\n",
    "def get_graphql_header():\n",
    "    return {\n",
    "    \"Authorization\": f\"Bearer {tokens[token_index]['token']}\",\n",
    "  }\n",
    "def get_rest_header():\n",
    "    return {\n",
    "    \"Accept\": \"application/vnd.github+json\",\n",
    "    \"Authorization\": f\"Bearer {tokens[token_index]['token']}\",\n",
    "    \"X-GitHub-Api-Version\": \"2022-11-28\"\n",
    "  }\n",
    "def send_rest_query(url, headers):\n",
    "    request = requests.get(url, headers=headers)\n",
    "    if request.status_code != 200:\n",
    "        logger.error(f\"{request.text}\")\n",
    "        raise QueryFailError(\n",
    "    \"Query failed to run by returning code of {}. {}\".format(\n",
    "        request.status_code, url\n",
    "    )\n",
    ")\n",
    "    response = json.JSONDecoder().decode(json.dumps(request.json(), sort_keys=True))\n",
    "    return response\n",
    "\n",
    "def search_rest_limit(headers):\n",
    "    try:\n",
    "        resp = send_rest_query(\"https://api.github.com/rate_limit\", headers)\n",
    "        global tokens\n",
    "        global token_index\n",
    "        tokens[token_index]['reset'] = pd.to_datetime(resp['resources']['search']['reset'])\n",
    "        if resp['resources']['search']['remaining'] > 1:\n",
    "            return\n",
    "        else:\n",
    "            token_index = (token_index + 1) % len(tokens)\n",
    "            if tokens[token_index]['reset'] is None:\n",
    "                return\n",
    "            if tokens[token_index]['reset'] < datetime.datetime.now(datetime.timezone.utc):\n",
    "                return\n",
    "            else:\n",
    "                time.sleep((tokens[token_index]['reset'] - datetime.datetime.now(datetime.timezone.utc)).total_seconds())\n",
    "                logger.info(f\"Token {token_index} is ready to use again.\")\n",
    "                return\n",
    "    except (QueryFailError,KeyError) as e:\n",
    "        logger.error('sleep in ' + str(datetime.datetime.now()) + str(e))\n",
    "        time.sleep(3600)\n",
    "\n",
    "def send_rest_query_with_retries(url, retries, headers):\n",
    "    request = requests.get(url, headers=headers)\n",
    "    if request.status_code != 200:\n",
    "        for i in range(retries):\n",
    "            time.sleep(random.randint(1, 3))\n",
    "            request = requests.get(url, headers=headers)\n",
    "            logger.info(\"retrying in \" + str (i +1))\n",
    "            if request.status_code == 200: \n",
    "                break\n",
    "            elif i == retries - 1:\n",
    "                logger.error(f\"{request.text}\")\n",
    "                raise QueryFailError(\n",
    "            \"Query failed to run by returning code of {}. {}\".format(\n",
    "                request.status_code, url\n",
    "            )\n",
    "        )\n",
    "    response = json.JSONDecoder().decode(json.dumps(request.json(), sort_keys=True))\n",
    "    return response\n",
    "        \n",
    "        \n",
    "graphql_url = \"https://api.github.com/graphql\"\n",
    "def send_graphql_query(query, headers):\n",
    "    request = requests.post(graphql_url, json={\"query\": query}, headers=headers)\n",
    "    if request.status_code != 200:\n",
    "        logger.error(f\"{request.text}\")\n",
    "        raise QueryFailError(\n",
    "    \"Query failed to run by returning code of {}. {}\".format(\n",
    "        request.status_code, query\n",
    "    )\n",
    ")\n",
    "    response = json.JSONDecoder().decode(json.dumps(request.json(), sort_keys=True))\n",
    "\n",
    "    try:\n",
    "        return response['data']\n",
    "    except KeyError:\n",
    "        return response\n",
    "    \n",
    "def search_graphql_limit(headers):\n",
    "    query = \"\"\"\n",
    "                    query {\n",
    "                      viewer {\n",
    "                        login\n",
    "                      }\n",
    "                      rateLimit {\n",
    "                        limit\n",
    "                        cost\n",
    "                        remaining\n",
    "                        resetAt\n",
    "                      }\n",
    "                }\n",
    "                \"\"\"\n",
    "    try:\n",
    "        resp = send_graphql_query(query, headers)\n",
    "        global tokens\n",
    "        global token_index\n",
    "        tokens[token_index]['reset'] = pd.to_datetime(resp['rateLimit']['resetAt'])\n",
    "        if resp['rateLimit']['remaining'] > 200:\n",
    "            return\n",
    "        else:\n",
    "            token_index = (token_index + 1) % len(tokens)\n",
    "            if tokens[token_index]['reset'] is None:\n",
    "                return\n",
    "            if tokens[token_index]['reset'] < datetime.datetime.now(datetime.timezone.utc):\n",
    "                return\n",
    "            else:\n",
    "                time.sleep((tokens[token_index]['reset'] - datetime.datetime.now(datetime.timezone.utc)).total_seconds())\n",
    "                print(f\"Token {token_index} is ready to use again.\")\n",
    "                return\n",
    "    except (QueryFailError,KeyError) as e:\n",
    "        logger.error('sleep in ' + str(datetime.datetime.now()) + str(e))\n",
    "        time.sleep(3600)\n",
    "        \n",
    "def send_graphql_query_with_retries(query, retries, headers):\n",
    "    request = requests.post(graphql_url, json={\"query\": query}, headers=headers)\n",
    "    if request.status_code != 200:\n",
    "        for i in range(retries):\n",
    "            time.sleep(random.randint(1, 3))\n",
    "            request = requests.post(graphql_url, json={\"query\": query}, headers=headers)\n",
    "            print(\"retrying in \", i +1)\n",
    "            if request.status_code == 200: \n",
    "                break\n",
    "            elif i == retries - 1:\n",
    "                logger.error(f\"{request.text}\")\n",
    "                raise QueryFailError(\n",
    "            \"Query failed to run by returning code of {}. {}\".format(\n",
    "                request.status_code, query\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    response = json.JSONDecoder().decode(json.dumps(request.json(), sort_keys=True))\n",
    "    \n",
    "    try:\n",
    "        return response['data']\n",
    "    except KeyError:\n",
    "        return response\n",
    "def clean_string(text):\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9.\\']+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0761ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e466f2-994f-4f9a-aca2-a9a6c1e33ddc",
   "metadata": {},
   "source": [
    "# Clone repositories for tracking the correct PR template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdcd8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in coding_df['repoName'].unique():\n",
    "    repo_name = repo.replace('/','_')\n",
    "    process = os.popen(f\"git clone https://github.com/{repo} {repo_name}\")\n",
    "    process.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf773ac-110a-4a7c-927e-ce9086928349",
   "metadata": {},
   "source": [
    "# Section 3.3 Revisions of PR Descriptions Generated by Copilot for PRs - Identifying the PR Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36401838",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_templates = pd.DataFrame()\n",
    "repos = []\n",
    "file_paths = []\n",
    "request = 0\n",
    "for repo in coding_df['repoName'].unique():\n",
    "    search_graphql_limit(get_graphql_header())\n",
    "    query = '''\n",
    "         query{\n",
    "          repository(name: \"repo_name\", owner: \"repo_owner\") {\n",
    "            pullRequestTemplates {\n",
    "              filename\n",
    "            }\n",
    "          }\n",
    "        }'''.replace('repo_name',repo.split('/')[1]).replace('repo_owner',repo.split('/')[0])\n",
    "    try:\n",
    "        results = send_graphql_query_with_retries(query,3,get_graphql_header())\n",
    "    except QueryFailError as e:\n",
    "        print(e)\n",
    "    templates = list(set([template['filename'] for template in results['repository']['pullRequestTemplates']]))\n",
    "    for template in results['repository']['pullRequestTemplates']:\n",
    "        search_rest_limit(get_rest_header())\n",
    "        url = f\"https://api.github.com/search/code?q=repo:{repo}+filename:{template['filename']}\"\n",
    "        try:\n",
    "            request += 1\n",
    "            if request > 8: \n",
    "                time.sleep(61)\n",
    "                request = 0\n",
    "            results = send_rest_query(url, get_rest_header())\n",
    "        except QueryFailError as e:\n",
    "            print(e)\n",
    "        for file in results['items']:\n",
    "            repos.append(repo)\n",
    "            file_paths.append(file['path'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfe72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_templates['repo'] = repos\n",
    "pr_templates['templateFilePath'] = file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ae924",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_templates[pr_templates.duplicated(['repo'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083244f-d8c5-4afd-8bb4-70d6a78a4c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_template_contents = []\n",
    "for name, group in coding_df.groupby(['repoName','number']):\n",
    "    repo_name = group.iloc[0]['repoName']\n",
    "    templates_file_paths = list(set(pr_templates[pr_templates['repo'] == repo_name]['templateFilePath']))\n",
    "    first_edit_time = pd.to_datetime(group.iloc[0]['editedAt'])\n",
    "    pr_des = group.iloc[0]['content']\n",
    "    contents = []\n",
    "    for file_path in templates_file_paths:\n",
    "        temp_repo_name = repo_name.replace('/','_')\n",
    "        command = [\n",
    "            'git', 'log', '--pretty=format:%H|%cd|', '--date=format:%Y-%m-%dT%H:%M:%S%z', '--name-only', '--follow', '--', file_path\n",
    "        ]\n",
    "        with subprocess.Popen(command, cwd=temp_repo_name, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as proc:\n",
    "            stdout, stderr = proc.communicate()\n",
    "            commits = stdout.decode('utf-8').strip().split('\\n\\n')\n",
    "            no_template = False\n",
    "            for index, commit in enumerate(commits):\n",
    "                commit = commit.replace('\\n','')\n",
    "                log = commit.split('|')\n",
    "                sha, commit_date, filepath = log[0], pd.to_datetime(log[1]), log[2]\n",
    "                if commit_date < first_edit_time:\n",
    "                    break\n",
    "                if index == len(commits) - 1:\n",
    "                    no_template = True\n",
    "            if not no_template:\n",
    "                command = [\n",
    "                    'git', 'show', f'{sha}:{filepath}'\n",
    "                ]\n",
    "                with subprocess.Popen(command, cwd=temp_repo_name, stdout=subprocess.PIPE, stderr=subprocess.PIPE) as proc:\n",
    "                    stdout, stderr = proc.communicate()\n",
    "                    content = stdout.decode('utf-8')\n",
    "                    contents.append(content)\n",
    "    if len(contents) == 1:\n",
    "        pr_template_contents.extend([contents[0]] * group.shape[0])\n",
    "    elif len(contents) == 0:\n",
    "        pr_template_contents.extend([None] * group.shape[0])\n",
    "    else:\n",
    "        contents.append(pr_des)\n",
    "        cleaned_template = [clean_string(content) for content in contents]\n",
    "        embeddings = model.encode(cleaned_template)\n",
    "        csim = cosine_similarity(embeddings)\n",
    "        pr_template_contents.extend([contents[list(csim[-1][:-1]).index(max(csim[-1][:-1]))]] * group.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68116c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_df['prTemplate'] = pr_template_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21436dc5-34db-434f-aae8-4f1babf3de1c",
   "metadata": {},
   "source": [
    "# 4.3 Qualitative Analysis - Preparing sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "groups = [coding_df for _, coding_df in coding_df.groupby('group')]\n",
    "random.shuffle(groups)\n",
    "\n",
    "df = pd.concat(groups).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5fabda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/coded_sample.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
